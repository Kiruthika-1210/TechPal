# ğŸ¤– TechPal â€“ Role-Adaptive AI Tech Assistant

TechPal is a **role-aware AI assistant** built using **Streamlit, LangChain, and Ollama**.  
It dynamically adapts its responses for **Developers, Admins, Students, and General Users**, making it a smart companion for coding help, system administration, learning, and everyday tech queries.

---

## ğŸš€ Key Features

- ğŸ§  **Role-Based Intelligence**
  - **Developer** â†’ Clean, executable, best-practice code
  - **Admin** â†’ DevOps, security, deployment commands
  - **Student** â†’ Beginner-friendly explanations
  - **User** â†’ Simple, non-technical guidance

- ğŸ’¬ **Multi-Turn Conversational Memory**
  - Maintains chat context using `ConversationBufferMemory`
  - Supports natural, continuous conversations

- âš¡ **Local LLM with Ollama**
  - Runs fully locally using models like `llama2`
  - No cloud dependency or API keys required

- ğŸ¨ **Modern Chat UI**
  - Streamlit-based chat interface
  - Role-specific greetings and icons
  - Scrollable chat container

- ğŸ”„ **Session-Safe Conversations**
  - Unique conversation IDs
  - Clear chat functionality

---

## ğŸ›  Tech Stack

| Layer | Technology |
|------|-----------|
| Frontend | Streamlit |
| LLM Orchestration | LangChain |
| Model Runtime | Ollama |
| Memory | ConversationBufferMemory |
| Prompt Engineering | ChatPromptTemplate |
| Language | Python 3.x |

---

## ğŸ“‚ Project Structure

TechPal/
â”œâ”€â”€ app.py # Main Streamlit application
â”œâ”€â”€ .env # Environment variables
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation


---

## âš™ï¸ Setup & Installation

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/Kiruthika-1210/TechPal.git
cd TechPal
```

### 2ï¸âƒ£ Install Dependencies
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Install & Run Ollama
```bash
ollama pull llama2
```

### 4ï¸âƒ£ Configure Environment Variables
Create a .env file:
```bash
MODEL_NAME=llama2
```

### 5ï¸âƒ£ Run TechPal
```bash
streamlit run app.py
```

---

## ğŸ§© How TechPal Works

1. User selects a role (**Developer / Admin / Student / User**)
2. A dynamic system prompt is generated based on the selected role
3. LangChain pipeline processes the request:
   - Prompt Template
   - Ollama LLM
   - Output Parser
4. Conversation memory maintains multi-turn context
5. Streamlit UI renders role-aware responses

---

## ğŸ§  Prompt Engineering Highlights

- Enforces **fully executable code** in Developer mode
- Provides **security warnings and best practices** in Admin mode
- Uses **simple explanations and examples** in Student mode
- Avoids technical jargon in User mode

---

## ğŸ“ˆ Future Enhancements

- API mode using FastAPI
- Conversation export (PDF / JSON)
- Vector-based memory using embeddings
- Cloud deployment (Render / AWS)
- Authentication and role-based access

---

## ğŸ¯ Resume Value

- Demonstrates real-world **LLM orchestration**
- Strong example of **prompt engineering**
- Shows **state management and memory handling**
- Combines **AI, frontend, and UX design**
- Easily extensible to production systems

---

## ğŸ“„ License

Open-source project. Free to use, modify, and extend.

---

## ğŸ‘©â€ğŸ’» Author

**Kiruthika (Kittu)**  
Aspiring Software Development Engineer | AI & Full-Stack Enthusiast
